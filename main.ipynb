{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class theData:\n",
    "    \"\"\"\n",
    "        theData class implements all required methods to read the input data and perform any processing methods.\n",
    "        \n",
    "        Class parameters:\n",
    "            xRaw: Actual inputs\n",
    "            xTitles: Label per input dimension\n",
    "            \n",
    "            xParsed: Covnert non numerical input to numerical (index of related label)\n",
    "            xLabels: The unique labels used in the input features (xRaw) - if xParced[i] = j, then xRaw[i] = Labels[j]\n",
    "\n",
    "            cutoff: Threshold used to convert output data to categorical (1/0)\n",
    "\n",
    "            tTitles: Label of each output dimension\n",
    "            tRaw: Raw output vector (MFIs)\n",
    "            tParsed: Output coverted to categorical (1/0) - tParsed[i] = tRaw[i]>=cutoff ? 1 : 0\n",
    "            \n",
    "        Supported methods (public):\n",
    "            readData: Import all the data (X and y)\n",
    "            setNewCutoff: Recompute tParsed with a new cutoff\n",
    "            \n",
    "    \"\"\"\n",
    "    def __init__(self, _defaultCutoff = 1200):\n",
    "        #initialize properties\n",
    "        self.xRaw = []\n",
    "        self.xTitles = []\n",
    "        self.xLabels = []\n",
    "        self.xParsed = []\n",
    "\n",
    "        self.cutoff = _defaultCutoff\n",
    "\n",
    "        self.tTitles = []\n",
    "        self.tRaw = []\n",
    "        self.tParsed = []\n",
    "\n",
    "    \"\"\"\n",
    "        Method: readData\n",
    "    \"\"\" \n",
    "    def readData(self, _path,  _inputCols, _outputCols, _sheetToRead=0):\n",
    "        \"\"\"\n",
    "            Read an excel/csv file to extract the table data and produce the features.\n",
    "            Parse data and convert non numerical data to numerical. Patients' records are stored horizontaly (Rows)\n",
    "            \n",
    "            _path (str): FULL path of the excel file to read - Read permission is assumed\n",
    "            _inputCols (str): Columns to read and produce the input vector, eg. D:AF meaning read cols from D to AF\n",
    "            _outputCols (str): Columns to read and produce the output vector. \n",
    "                               IMPORTANT: Each column will be a independent categorization -> experiment\n",
    "            _sheetToRead (int): Which sheet to read (1 for the first sheet, etc.)\n",
    "        \"\"\"\n",
    "        #\n",
    "        # INPUT\n",
    "        #\n",
    "        df = pd.read_excel(io=_path, header=0, usecols=_inputCols, sheet_name=_sheetToRead, dtype=str)\n",
    "        df.fillna('0', inplace=True)\n",
    "        self.xRaw = df.values  # input\n",
    "        self.xTitles = df.columns\n",
    "        # Convert input to numerical data (-> replace with index of the labels matrix)\n",
    "        T = np.concatenate(self.xRaw, axis=0)\n",
    "        self.xLabels = np.unique(T)\n",
    "        self.xParsed = np.copy(self.xRaw)\n",
    "        for i in range(0,self.xLabels.shape[0]):\n",
    "            self.xParsed[self.xParsed == self.xLabels[i]] = i\n",
    "\n",
    "        #\n",
    "        # OUTPUT\n",
    "        #\n",
    "        df = pd.read_excel(io=_path, header=0, usecols=_outputCols, sheet_name=_sheetToRead, dtype=int)\n",
    "        df.fillna(-1, inplace=True)\n",
    "        self.tRaw = df.values  # input\n",
    "        self.tTitles = df.columns\n",
    "        # Convert continues data to categorical (TRUE/FALSE)\n",
    "        self.tParsed = np.copy(self.tRaw)\n",
    "        self.tParsed[self.tRaw >= self.cutoff] = 1\n",
    "        self.tParsed[self.tRaw <  self.cutoff] = 0\n",
    "    \n",
    "    \"\"\"\n",
    "        Method: setNewCutoff\n",
    "    \"\"\" \n",
    "    def setNewCutoff(self, _newCutoff):\n",
    "        \"\"\"\n",
    "            Set a new cutoff and reassign categories\n",
    "        \"\"\"\n",
    "        self.cutoff = _newCutoff\n",
    "        self.tParsed = np.copy(self.tRaw)\n",
    "        self.tParsed[self.tRaw >= self.cutoff] = 1\n",
    "        self.tParsed[self.tRaw <  self.cutoff] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = theData(1200)\n",
    "dt.readData(\"./data/LSA1_DATA.xlsx\", \"G:L\", \"S:DK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "class theStudy:\n",
    "    \"\"\"\n",
    "        theStudy class implements all the analysis proposed in the related paper.\n",
    "        \n",
    "        Class parameters:\n",
    "            data: The input data (observations + labels)\n",
    "            cvTestSize: Percentage of test size in a fold of a k-fold cross validation\n",
    "            cvIterations: How many iterations to perform of a k-fold cross validation\n",
    "            \n",
    "        Supported methods (public):\n",
    "            prepareCrossValidation: Prepare the Cross Validation Tests\n",
    "            doAnalyze: Perform the prediction analysis\n",
    "    \"\"\"\n",
    "    def __init__(self, _data, _cvTestSize=.3, _cvIterations=10):\n",
    "        #initialize dataset\n",
    "        self.data = _data        \n",
    "        #initialize cross validation\n",
    "        self.prepareCrossValidation(_cvTestSize, _cvIterations)\n",
    "        \n",
    "    \"\"\"\n",
    "        Method: prepareCrossValidation\n",
    "    \"\"\" \n",
    "    def prepareCrossValidation(self, _cvTestSize=.3, _cvIterations=10):\n",
    "        \"\"\"\n",
    "            Prepare the Cross Validation Tests\n",
    "            \n",
    "            _cvTestSize (float in (0,1)): Percentage of test size in a fold of a k-fold cross validation\n",
    "            _cvIterations (int in [1,10]): How many iterations to perform of a k-fold cross validation\n",
    "        \"\"\"\n",
    "        #initialize cross validation configuration\n",
    "        if _cvTestSize<=0 or _cvTestSize >=1:\n",
    "            _cvTestSize = .3\n",
    "        _cvIterations = int(_cvIterations)\n",
    "        if _cvIterations<=0 or _cvTestSize >=10:\n",
    "            _cvTestSize = 10\n",
    "        self.cvTestSize = _cvTestSize\n",
    "        self.cvIterations = _cvIterations\n",
    "        self.cv = ShuffleSplit(n_splits=_cvIterations, test_size=_cvTestSize, random_state=0)\n",
    "    \"\"\"\n",
    "        Method: doAnalyze\n",
    "    \"\"\" \n",
    "    def doAnalyze(self, _mfiCol, _data=None, _clf=None):\n",
    "        \"\"\"\n",
    "            Perform the prediction analysis\n",
    "            \n",
    "            _mfiCol (int): Which column to analyze\n",
    "            _data (class theData): Reassign input data.\n",
    "        \"\"\"\n",
    "        if not (_data is None):\n",
    "            self.data = _data\n",
    "        # initialize classifier\n",
    "        if _clf is None:\n",
    "            clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20, 3), max_iter=5000, random_state=0)\n",
    "        else:\n",
    "            clf = _clf\n",
    "        # Cross Validation -> compute fitting scores\n",
    "        precission = np.zeros((len(_mfiCol), self.cvIterations))\n",
    "        recall = np.zeros((len(_mfiCol), self.cvIterations))\n",
    "        f1 = np.zeros((len(_mfiCol), self.cvIterations))\n",
    "        for i in range(0,len(_mfiCol)):\n",
    "            precission[i,:] = cross_val_score(clf, self.data.xParsed, self.data.tParsed[:,_mfiCol[i]], cv=self.cv, scoring='accuracy')\n",
    "            recall[i,:] = cross_val_score(clf, self.data.xParsed, self.data.tParsed[:,_mfiCol[i]], cv=self.cv, scoring='recall')\n",
    "            f1[i,:] = cross_val_score(clf, self.data.xParsed, self.data.tParsed[:,_mfiCol[i]], cv=self.cv, scoring='f1')\n",
    "        return(precission, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1000: A*01:01 - P/R/F1: 0.81/0.00/0.00\n",
      "1000: A*02:01 - P/R/F1: 0.85/0.00/0.00\n",
      "1000: A*02:03 - P/R/F1: 0.83/0.00/0.00\n",
      "1000: A*02:06 - P/R/F1: 0.84/0.00/0.00\n",
      "1000: A*03:01 - P/R/F1: 0.87/0.00/0.00\n",
      "1000: A*11:01 - P/R/F1: 0.88/0.00/0.00\n",
      "-------------------------------------------\n",
      "1100: A*01:01 - P/R/F1: 0.81/0.00/0.00\n",
      "1100: A*02:01 - P/R/F1: 0.80/0.03/0.06\n",
      "1100: A*02:03 - P/R/F1: 0.85/0.00/0.00\n",
      "1100: A*02:06 - P/R/F1: 0.85/0.00/0.00\n",
      "1100: A*03:01 - P/R/F1: 0.88/0.00/0.00\n",
      "1100: A*11:01 - P/R/F1: 0.88/0.00/0.00\n",
      "-------------------------------------------\n",
      "1200: A*01:01 - P/R/F1: 0.81/0.00/0.00\n",
      "1200: A*02:01 - P/R/F1: 0.86/0.00/0.00\n",
      "1200: A*02:03 - P/R/F1: 0.85/0.00/0.00\n",
      "1200: A*02:06 - P/R/F1: 0.86/0.00/0.00\n",
      "1200: A*03:01 - P/R/F1: 0.88/0.00/0.00\n",
      "1200: A*11:01 - P/R/F1: 0.88/0.00/0.00\n",
      "-------------------------------------------\n",
      "1300: A*01:01 - P/R/F1: 0.82/0.00/0.00\n",
      "1300: A*02:01 - P/R/F1: 0.86/0.00/0.00\n",
      "1300: A*02:03 - P/R/F1: 0.86/0.00/0.00\n",
      "1300: A*02:06 - P/R/F1: 0.87/0.00/0.00\n",
      "1300: A*03:01 - P/R/F1: 0.89/0.00/0.00\n",
      "1300: A*11:01 - P/R/F1: 0.89/0.00/0.00\n",
      "-------------------------------------------\n",
      "1400: A*01:01 - P/R/F1: 0.83/0.00/0.00\n",
      "1400: A*02:01 - P/R/F1: 0.87/0.00/0.00\n",
      "1400: A*02:03 - P/R/F1: 0.87/0.00/0.00\n",
      "1400: A*02:06 - P/R/F1: 0.83/0.02/0.04\n",
      "1400: A*03:01 - P/R/F1: 0.89/0.00/0.00\n",
      "1400: A*11:01 - P/R/F1: 0.89/0.00/0.00\n",
      "-------------------------------------------\n",
      "1500: A*01:01 - P/R/F1: 0.84/0.00/0.00\n",
      "1500: A*02:01 - P/R/F1: 0.88/0.00/0.00\n",
      "1500: A*02:03 - P/R/F1: 0.87/0.00/0.00\n",
      "1500: A*02:06 - P/R/F1: 0.88/0.00/0.00\n",
      "1500: A*03:01 - P/R/F1: 0.89/0.00/0.00\n",
      "1500: A*11:01 - P/R/F1: 0.89/0.00/0.00\n",
      "-------------------------------------------\n",
      "1700: A*01:01 - P/R/F1: 0.84/0.00/0.00\n",
      "1700: A*02:01 - P/R/F1: 0.88/0.00/0.00\n",
      "1700: A*02:03 - P/R/F1: 0.87/0.00/0.00\n",
      "1700: A*02:06 - P/R/F1: 0.88/0.00/0.00\n",
      "1700: A*03:01 - P/R/F1: 0.90/0.00/0.00\n",
      "1700: A*11:01 - P/R/F1: 0.89/0.03/0.05\n",
      "-------------------------------------------\n",
      "2000: A*01:01 - P/R/F1: 0.85/0.00/0.00\n",
      "2000: A*02:01 - P/R/F1: 0.88/0.00/0.00\n",
      "2000: A*02:03 - P/R/F1: 0.88/0.00/0.00\n",
      "2000: A*02:06 - P/R/F1: 0.89/0.00/0.00\n",
      "2000: A*03:01 - P/R/F1: 0.91/0.00/0.00\n",
      "2000: A*11:01 - P/R/F1: 0.90/0.00/0.00\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cutoffs = [1000, 1100, 1200, 1300, 1400, 1500, 1700, 2000]\n",
    "\n",
    "mfis = [0,1,2,3,4,5]\n",
    "\n",
    "for j in range(0,len(cutoffs)):\n",
    "    dt.setNewCutoff(cutoffs[j])\n",
    "    study = theStudy(dt)\n",
    "    (P, R, F1) = study.doAnalyze(mfis)\n",
    "    for i in range(0,len(mfis)):\n",
    "        m_max = np.max(F1[i,:])\n",
    "        i_max = np.where(F1[i,:] == m_max)\n",
    "        i_max = i_max[0][0]\n",
    "\n",
    "        print('%d: %s - P/R/F1: %.2f/%.2f/%.2f' % (cutoffs[j], dt.tTitles[i], P[i,i_max], R[i,i_max], m_max))\n",
    "    print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}